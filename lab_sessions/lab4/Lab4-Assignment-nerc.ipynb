{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4-Assignment about Named Entity Recognition and Classification\n",
    "\n",
    "This notebook describes the assignment of Lab 4 of the text mining course. We assume you have succesfully completed Lab1, Lab2 and Lab3 as welll. Especially Lab2 is important for completing this assignment.\n",
    "\n",
    "**Learning goals**\n",
    "* going from linguistic input format to representing it in a feature space\n",
    "* working with pretrained word embeddings\n",
    "* train a supervised classifier (SVM)\n",
    "* evaluate a supervised classifier (SVM)\n",
    "* learn how to interpret the system output and the evaluation results\n",
    "* be able to propose future improvements based on the observed results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This notebook was originally created by [Marten Postma](https://martenpostma.github.io) and [Filip Ilievski](http://ilievski.nl) and adapted by Piek vossen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 18] Exercise 1 (NERC): Training and evaluating an SVM using CoNLL-2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 point] a) Load the CoNLL-2003 training data using the *ConllCorpusReader* and create for both *train.txt* and *test.txt*:**\n",
    "\n",
    "    [2 points]  -a list of dictionaries representing the features for each training instances, e..g,\n",
    "    ```\n",
    "    [\n",
    "    {'words': 'EU', 'pos': 'NNP'}, \n",
    "    {'words': 'rejects', 'pos': 'VBZ'},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    [2 points] -the NERC labels associated with each training instance, e.g.,\n",
    "    dictionaries, e.g.,\n",
    "    ```\n",
    "    [\n",
    "    'B-ORG', \n",
    "    'O',\n",
    "    ....\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "train = ConllCorpusReader('/Users/kamilpulchny/Desktop/Text Mining/ba-text-mining/lab_sessions/lab4/CONLL2003/CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "training_features = []\n",
    "training_gold_labels = []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    a_dict = {\n",
    "         'word': token,\n",
    "         'pos': pos\n",
    "    }\n",
    "    training_features.append(a_dict)\n",
    "    training_gold_labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "train = ConllCorpusReader('/Users/kamilpulchny/Desktop/Text Mining/ba-text-mining/lab_sessions/lab4/CONLL2003/CONLL2003', 'test.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "test_features = []\n",
    "test_gold_labels = []\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    a_dict = {\n",
    "        'word': token,\n",
    "        'pos': pos\n",
    "    }\n",
    "    test_features.append(a_dict)\n",
    "    test_gold_labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] b) provide descriptive statistics about the training and test data:**\n",
    "* How many instances are in train and test?\n",
    "* Answer: training data 203621, test data: 46435\n",
    "* Provide a frequency distribution of the NERC labels, i.e., how many times does each NERC label occur?\n",
    "* Answer:\n",
    "* Training NERC labels frequency distribution:\n",
    "B-ORG: 6321\n",
    "O: 169578\n",
    "B-MISC: 3438\n",
    "B-PER: 6600\n",
    "I-PER: 4528\n",
    "B-LOC: 7140\n",
    "I-ORG: 3704\n",
    "I-MISC: 1155\n",
    "I-LOC: 1157\n",
    "* Test NERC labels frequency distribution:\n",
    "O: 38323\n",
    "B-LOC: 1668\n",
    "B-PER: 1617\n",
    "I-PER: 1156\n",
    "I-LOC: 257\n",
    "B-MISC: 702\n",
    "I-MISC: 216\n",
    "B-ORG: 1661\n",
    "I-ORG: 835\n",
    "* Discuss to what extent the training and test data is balanced (equal amount of instances for each NERC label) and to what extent the training and test data differ?\n",
    "* Answer: The training and test data is balanced because data has a larger number of instances, 203621, compared to the test data, 46435, which is correct in order to get good training results. The training and test data differ in terms of the NERC labels frequency distribution. The training data has a higher frequency distribution of NERC labels compared to the test data which my influence validation later on.\n",
    "\n",
    "Tip: you can use the following `Counter` functionality to generate frequency list of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 2, 2: 2, 3: 1, 5: 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "\n",
    "my_list=[1,2,1,3,2,5]\n",
    "Counter(my_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] c) Concatenate the train and test features (the list of dictionaries) into one list. Load it using the *DictVectorizer*. Afterwards, split it back to training and test.**\n",
    "\n",
    "Tip: You’ve concatenated train and test into one list and then you’ve applied the DictVectorizer.\n",
    "The order of the rows is maintained. You can hence use an index (number of training instances) to split the_array back into train and test. Do NOT use: `\n",
    "from sklearn.model_selection import train_test_split` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "the_array = vec.fit_transform(training_features + test_features)\n",
    "\n",
    "num_train = len(training_features)\n",
    "train_array = the_array[:num_train]\n",
    "test_array = the_array[num_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] d) Train the SVM using the train features and labels and evaluate on the test data. Provide a classification report (sklearn.metrics.classification_report).**\n",
    "The train (*lin_clf.fit*) might take a while. On my computer, it took 1min 53s, which is acceptable. Training models normally takes much longer. If it takes more than 5 minutes, you can use a subset for training. Describe the results:\n",
    "* Which NERC labels does the classifier perform well on? Why do you think this is the case?\n",
    "* Which NERC labels does the classifier perform poorly on? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilpulchny/Library/Python/3.12/lib/python/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.81      0.77      0.79      1668\n",
      "      B-MISC       0.78      0.66      0.71       702\n",
      "       B-ORG       0.79      0.52      0.62      1661\n",
      "       B-PER       0.87      0.44      0.58      1617\n",
      "       I-LOC       0.62      0.53      0.57       257\n",
      "      I-MISC       0.59      0.59      0.59       216\n",
      "       I-ORG       0.66      0.48      0.55       835\n",
      "       I-PER       0.33      0.87      0.48      1156\n",
      "           O       0.99      0.98      0.98     38323\n",
      "\n",
      "    accuracy                           0.92     46435\n",
      "   macro avg       0.71      0.65      0.65     46435\n",
      "weighted avg       0.94      0.92      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### [ YOUR CODE SHOULD GO HERE ]\n",
    "from sklearn.metrics import classification_report\n",
    "lin_clf.fit(train_array, training_gold_labels)\n",
    "pred_test = lin_clf.predict(test_array)\n",
    "print(classification_report(test_gold_labels, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which NERC labels does the classifier perform well on? Why do you think this is the case?\n",
    "\n",
    "The classifier does remarkably well on B-LOC entities and the \"O\" label.  Due to the overwhelming frequency of non-entity tokens in the dataset, which makes them simpler to learn and predict accurately, the \"O\" label achieves precision of 0.99, recall of 0.98, and a f1-score of 0.98.  Similarly, location names typically display unique, consistent characteristics, like particular capitalization patterns and contextual cues, that help the model recognize them effectively, allowing the classifier to obtain a f1-score of 0.79 for B-LOC tokens.\n",
    "\n",
    "Which NERC labels does the classifier perform poorly on? Why do you think this is the case?\n",
    "\n",
    "In particular, the classifier has trouble with organization entities (B-ORG and I-ORG) and I-PER and B-PER labels.  Even though I-PER has a high recall of 0.87, its precision is extremely low at 0.33, resulting in a f1-score of 0.48. This suggests that there is a propensity to overpredict I-PER tokens because person name structures are unpredictable and ambiguous.  The B-PER model has a high precision of 0.87 but a low recall of 0.44, which means it misses a lot of real-world person name instances. This is probably because the patterns are varied and unpredictable.  Organization labels perform mediocrely as well, most likely due to the fact that organization names are more difficult to correctly categorize because they frequently lack the distinct, recognizable characteristics of location names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6 points] e) Train a model that uses the embeddings of these words as inputs. Test again on the same data as in 2d. Generate a classification report and compare the results with the classifier you built in 2d.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilpulchny/Library/Python/3.12/lib/python/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.81      0.77      0.79      1668\n",
      "      B-MISC       0.78      0.66      0.71       702\n",
      "       B-ORG       0.79      0.52      0.62      1661\n",
      "       B-PER       0.87      0.44      0.58      1617\n",
      "       I-LOC       0.62      0.53      0.57       257\n",
      "      I-MISC       0.59      0.59      0.59       216\n",
      "       I-ORG       0.66      0.48      0.55       835\n",
      "       I-PER       0.33      0.87      0.48      1156\n",
      "           O       0.99      0.98      0.98     38323\n",
      "\n",
      "    accuracy                           0.92     46435\n",
      "   macro avg       0.71      0.65      0.65     46435\n",
      "weighted avg       0.94      0.92      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import gensim.downloader as api\n",
    "\n",
    "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "\n",
    "def get_embedding(word):\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in fasttext_model:\n",
    "        return fasttext_model[word_lower]\n",
    "    else:\n",
    "        return np.zeros(fasttext_model.vector_size)\n",
    "\n",
    "train_embeddings = np.array([get_embedding(feat['word']) for feat in training_features])\n",
    "test_embeddings  = np.array([get_embedding(feat['word']) for feat in test_features])\n",
    "lin_clf = svm.LinearSVC()\n",
    "lin_clf.fit(train_array, training_gold_labels)\n",
    "pred_test = lin_clf.predict(test_array)\n",
    "\n",
    "print(classification_report(test_gold_labels, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 10] Exercise 2 (NERC): feature inspection using the [Annotated Corpus for Named Entity Recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)\n",
    "**[6 points] a. Perform the same steps as in the previous exercise. Make sure you end up for both the training part (*df_train*) and the test part (*df_test*) with:**\n",
    "* the features representation using **DictVectorizer**\n",
    "* the NERC labels in a list\n",
    "\n",
    "Please note that this is the same setup as in the previous exercise:\n",
    "* load both train and test using:\n",
    "    * list of dictionaries for features\n",
    "    * list of NERC labels\n",
    "* combine train and test features in a list and represent them using one hot encoding\n",
    "* train using the training features and NERC labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Adapt the path to point to your local copy of NERC_datasets\n",
    "path = r'C:\\Users\\krzys\\OneDrive\\Pulpit\\TextMining\\ba-text-mining\\lab_sessions\\lab4\\ner_dataset.csv'\n",
    "kaggle_dataset = pandas.read_csv(path, encoding='latin1', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048575"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kaggle_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 20000\n"
     ]
    }
   ],
   "source": [
    "df_train = kaggle_dataset[:100000]\n",
    "df_test = kaggle_dataset[100000:120000]\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DictVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Combine training and test features for a unified one-hot encoding using DictVectorizer\u001b[39;00m\n\u001b[0;32m     21\u001b[0m all_features \u001b[38;5;241m=\u001b[39m train_features \u001b[38;5;241m+\u001b[39m test_features\n\u001b[1;32m---> 22\u001b[0m dv \u001b[38;5;241m=\u001b[39m \u001b[43mDictVectorizer\u001b[49m(sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m X_all \u001b[38;5;241m=\u001b[39m dv\u001b[38;5;241m.\u001b[39mfit_transform(all_features)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Separate back into training and test feature matrices\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DictVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "def extract_features(row):\n",
    "    word = row['Word']\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'word.lower': word.lower(),\n",
    "        'is_upper': word.isupper(),\n",
    "        'is_title': word.istitle(),\n",
    "        'is_digit': word.isdigit()\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Create a list of feature dictionaries for the training and test sets\n",
    "train_features = df_train.apply(extract_features, axis=1).tolist()\n",
    "test_features = df_test.apply(extract_features, axis=1).tolist()\n",
    "\n",
    "# Extract the NERC labels into lists (assuming the column 'Tag' contains the labels)\n",
    "train_labels = df_train['Tag'].tolist()\n",
    "test_labels = df_test['Tag'].tolist()\n",
    "\n",
    "# Combine training and test features for a unified one-hot encoding using DictVectorizer\n",
    "all_features = train_features + test_features\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X_all = dv.fit_transform(all_features)\n",
    "\n",
    "# Separate back into training and test feature matrices\n",
    "X_train = X_all[:len(train_features)]\n",
    "X_test = X_all[len(train_features):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] b. Train and evaluate the model and provide the classification report:**\n",
    "* use the SVM to predict NERC labels on the test data\n",
    "* evaluate the performance of the SVM on the test data\n",
    "\n",
    "Analyze the performance per NERC label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
